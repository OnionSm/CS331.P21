import streamlit as st
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json
import cv2
from ultralytics import YOLO
from facenet_pytorch import InceptionResnetV1
from PIL import Image
from torchvision import transforms
import torch

st.title("HUMAN FACE DETECTION")

# Ch·ªâ ƒë·ªçc database m·ªôt l·∫ßn v√† cache l·∫°i
@st.cache_resource
def read_face_db():
    with open("face_db.json", "r") as f:
        face_db = json.load(f)

    # Chuy·ªÉn l·∫°i t·ª´ list -> numpy array
    face_db = {
        k: [np.array(vec) for vec in v]
        for k, v in face_db.items()
    }
    return face_db

# H√†m so s√°nh ·∫£nh m·ªõi v·ªõi DB
def query_database(vec_new, db, threshold=0.8):
    
    best_score = -1
    best_name = "Unknown"

    for name, vec_list in db.items():
        for vec in vec_list:
            score = cosine_similarity([vec_new], [vec])[0][0]
            if score > best_score:
                best_score = score
                best_name = name

    if best_score >= threshold:
        return best_name, best_score
    else:
        return "Unknown", best_score
    

def get_crop_image_with_yolo(model, image):
    results = model.predict(image) 
    result = results[0]

    # N·∫øu kh√¥ng c√≥ box n√†o ƒë∆∞·ª£c d·ª± ƒëo√°n
    if len(result.boxes) == 0:
        return None, []

    # T√¨m box c√≥ confidence cao nh·∫•t
    best_box = max(result.boxes, key=lambda box: float(box.conf[0]))

    x1, y1, x2, y2 = map(int, best_box.xyxy[0])
    conf = float(best_box.conf[0])
    cls = int(best_box.cls[0])
    label = f"{model.names[cls]} {conf:.2f}"

    # V·∫Ω bounding box v√† nh√£n
    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 1)
    cv2.putText(image, label, (x1, y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    # Crop ·∫£nh theo bounding box t·ªët nh·∫•t
    image_crop = image[y1:y2, x1:x2]
    image_crop = cv2.cvtColor(image_crop, cv2.COLOR_BGR2RGB)

    # Chuy·ªÉn ·∫£nh g·ªëc sang RGB ƒë·ªÉ hi·ªÉn th·ªã
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    return image_rgb, image_crop

def save_data_to_database(face_db):
    serializable_face_db = {
    k: [arr.tolist() for arr in v] 
    for k, v in face_db.items()
    }
    with open("face_db.json", "w") as f:
        json.dump(serializable_face_db, f, indent=2)
    

def get_embedding_from_numpy(facenet_model, image_np: np.ndarray) -> np.ndarray:
    """
    Nh·∫≠n ·∫£nh numpy RGB, tr·∫£ v·ªÅ vector 512 chi·ªÅu t·ª´ Facenet.
    """
    transform = transforms.Compose([
        transforms.Resize((160, 160)),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])

    if image_np is None:
        return None

    # N·∫øu ·∫£nh l√† BGR (OpenCV), chuy·ªÉn sang RGB
    if image_np.shape[2] == 3:
        image_rgb = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)
    else:
        image_rgb = image_np

    # Resize ·∫£nh v·ªÅ 160x160
    image_resized = cv2.resize(image_rgb, (160, 160))

    # Normalize v√† chuy·ªÉn v·ªÅ tensor
    image_tensor = transforms.ToTensor()(image_resized)
    image_tensor = transforms.Normalize([0.5], [0.5])(image_tensor)
    image_tensor = image_tensor.unsqueeze(0)  # B x C x H x W

    # D·ª± ƒëo√°n
    with torch.no_grad():
        embedding = facenet_model(image_tensor)
    return embedding.squeeze().numpy() 



@st.cache_resource
def load_detection_model(model_path):
    return YOLO("best.pt")

@st.cache_resource
def load_embedding_model(model_pretrained):
    facenet_model = InceptionResnetV1(pretrained=model_pretrained).eval()
    return facenet_model

# Load database
face_db = read_face_db()
detection_model = load_detection_model("best.pt")
embedding_model = load_embedding_model("vggface2")



# Kh·ªüi t·∫°o tr·∫°ng th√°i hi·ªÉn th·ªã n·∫øu ch∆∞a c√≥
if "show_form" not in st.session_state:
    st.session_state.show_form = False

# N√∫t b·∫•m "Th√™m ng∆∞·ªùi m·ªõi"
if st.button("‚ûï Th√™m ng∆∞·ªùi m·ªõi"):
    st.session_state.show_form = True

# Ch·ªâ hi·ªÉn th·ªã form khi n√∫t ƒë∆∞·ª£c b·∫•m
if st.session_state.show_form:
    st.subheader("üìù Nh·∫≠p th√¥ng tin ng∆∞·ªùi m·ªõi")

    # Upload nhi·ªÅu ·∫£nh
    uploaded_files = st.file_uploader("Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu ·∫£nh", type=["jpg", "jpeg", "png"], accept_multiple_files=True)

    # Nh·∫≠p t√™n
    name_input = st.text_input("‚úçÔ∏è Nh·∫≠p t√™n c·ªßa b·∫°n")

    # N√∫t x·ª≠ l√Ω
    if st.button("üöÄ X·ª≠ l√Ω ·∫£nh"):
        if uploaded_files and name_input:
            st.success(f"T√™n b·∫°n nh·∫≠p: {name_input}")
            new_data_human = []
            for idx, uploaded_file in enumerate(uploaded_files):
                image = Image.open(uploaded_file)
                #st.image(image, caption=f"·∫¢nh {idx+1}", use_column_width=True)
                st.info(f"·∫¢nh {idx+1} ƒë√£ ƒë∆∞·ª£c n·∫°p th√†nh c√¥ng!")
                image_np = np.array(image)
                if image_np.shape[2] == 4:
                    image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)
                print(image_np.shape)
                _, query_image = get_crop_image_with_yolo(detection_model, image_np)
                query_embedding = get_embedding_from_numpy(embedding_model, query_image)
                new_data_human.append(query_embedding)
                
            if name_input in face_db:
                face_db[name_input].extend(new_data_human)
            else:
                face_db[name_input] = new_data_human
            save_data_to_database(face_db)

        else:
            st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt ·∫£nh v√† nh·∫≠p t√™n.")


# Giao di·ªán upload ·∫£nh
uploaded_file = st.file_uploader("Ch·ªçn m·ªôt ·∫£nh ƒë·ªÉ upload", type=["jpg", "jpeg", "png"])

# X·ª≠ l√Ω v√† hi·ªÉn th·ªã ·∫£nh
if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption="·∫¢nh b·∫°n ƒë√£ upload", use_column_width=True)

    image = np.array(image)
    if image.shape[2] == 4:
        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)

    # Tr√≠ch khu√¥n m·∫∑t v√† embedding
    _, query_image = get_crop_image_with_yolo(detection_model, image)
    query_embedding = get_embedding_from_numpy(embedding_model, query_image)

    # So kh·ªõp v·ªõi database
    name, score = query_database(query_embedding, face_db)

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ l√™n giao di·ªán
    st.write(f"**T√™n d·ª± ƒëo√°n:** {name}")
    st.write(f"**ƒê·ªô t∆∞∆°ng ƒë·ªìng:** {score:.4f}")

